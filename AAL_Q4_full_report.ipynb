{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeabe4dc",
   "metadata": {},
   "source": [
    "\n",
    "# AAL — Q4 2020 Sales Analysis (Full Report)\n",
    "\n",
    "**Purpose:** Provide an in-depth analysis of AAL's fourth-quarter sales across Australia, by state and group, and produce actionable recommendations for the Head of Sales & Marketing.\n",
    "\n",
    "**Contents:**\n",
    "1. Data loading and inspection  \n",
    "2. Data wrangling (cleaning, missing-value treatment)  \n",
    "3. Normalization (Min–Max)  \n",
    "4. Descriptive statistics (Sales, Unit)  \n",
    "5. Group-wise and State-wise analysis  \n",
    "6. Time-based analysis (daily/weekly/monthly/quarterly)  \n",
    "7. Time-of-day analysis (hourly)  \n",
    "8. Visualizations and dashboard-style plots  \n",
    "9. Recommendations and next steps\n",
    "\n",
    "_This notebook was auto-generated. Run all cells in order to reproduce the analysis._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb931af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load libraries and the dataset\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sns.set()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "csv_path = r\"/mnt/data/AusApparalSales4thQrt2020.csv\"\n",
    "print(\"Loading:\", csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81bfb0",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Initial Inspection\n",
    "\n",
    "We check column types, missing values, and a quick peek at distributions for numeric fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ff7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic info and missing values\n",
    "display(df.info())\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (df.isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "pd.concat([missing, missing_pct], axis=1).rename(columns={0:'missing_count',1:'missing_pct'}) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc69a7",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Data Wrangling\n",
    "\n",
    "**Steps performed:**\n",
    "- Standardize column names (trim spaces)\n",
    "- Parse `Date` and `Time` where possible\n",
    "- Fill or flag missing values (if any)\n",
    "- Create helpful date/time features: `Date` (datetime), `hour`, `dayofweek`\n",
    "- Add normalization columns (`Sales_norm`, `Unit_norm`) using Min–Max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31903cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copy and standardize column names\n",
    "df_clean = df.copy()\n",
    "df_clean.columns = [c.strip() for c in df_clean.columns]\n",
    "\n",
    "# Attempt to parse Date column(s)\n",
    "date_col = None\n",
    "for c in df_clean.columns:\n",
    "    if 'date' in c.lower() or 'timestamp' in c.lower():\n",
    "        date_col = c\n",
    "        break\n",
    "# If exact Date column not detected by name, try to parse first object column that looks like dates\n",
    "if date_col is None:\n",
    "    for c in df_clean.columns:\n",
    "        if df_clean[c].dtype == object:\n",
    "            parsed = pd.to_datetime(df_clean[c], errors='coerce')\n",
    "            if parsed.notna().sum() > 0:\n",
    "                date_col = c\n",
    "                break\n",
    "\n",
    "if date_col:\n",
    "    df_clean['Date'] = pd.to_datetime(df_clean[date_col], errors='coerce')\n",
    "else:\n",
    "    df_clean['Date'] = pd.NaT\n",
    "\n",
    "# Time column handling (if exists)\n",
    "time_col = None\n",
    "for c in df_clean.columns:\n",
    "    if c.lower() == 'time' or 'time' in c.lower():\n",
    "        time_col = c\n",
    "        break\n",
    "if time_col:\n",
    "    df_clean['Time'] = pd.to_datetime(df_clean[time_col], errors='coerce').dt.time\n",
    "\n",
    "# Standardize category columns\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype == object:\n",
    "        df_clean[col] = df_clean[col].str.strip()\n",
    "\n",
    "# Identify expected numeric columns\n",
    "possible_sales = [c for c in df_clean.columns if c.lower() in ['sales','sale','amount','revenue','total']]\n",
    "possible_units = [c for c in df_clean.columns if c.lower() in ['unit','units','qty','quantity']]\n",
    "\n",
    "sales_col = possible_sales[0] if possible_sales else None\n",
    "unit_col  = possible_units[0] if possible_units else None\n",
    "\n",
    "print(\"Detected sales column:\", sales_col)\n",
    "print(\"Detected unit column:\", unit_col)\n",
    "\n",
    "# Summary of missing values\n",
    "missing = pd.concat([df_clean.isna().sum(), (df_clean.isna().mean()*100).round(2)], axis=1)\n",
    "missing.columns = ['missing_count','missing_pct']\n",
    "missing.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca18378",
   "metadata": {},
   "source": [
    "\n",
    "### Missing-data policy (applied conditionally)\n",
    "\n",
    "- **Numeric (Sales/Unit):** fill with median (robust) OR infer from similar records (State+Group+Date) when viable. We'll use median fill only if missing values exist.  \n",
    "- **Categorical (State/Group):** fill with `'Unknown'` and keep for investigation.  \n",
    "- **Date/Time missing:** flag and exclude those rows from time-series analyses but include them in totals if Sales exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6233eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply recommended fills if needed\n",
    "if sales_col and df_clean[sales_col].isna().any():\n",
    "    median_sales = df_clean[sales_col].median()\n",
    "    df_clean[sales_col] = df_clean[sales_col].fillna(median_sales)\n",
    "    print(\"Filled Sales nulls with median:\", median_sales)\n",
    "if unit_col and df_clean[unit_col].isna().any():\n",
    "    median_units = df_clean[unit_col].median()\n",
    "    df_clean[unit_col] = df_clean[unit_col].fillna(median_units)\n",
    "    print(\"Filled Units nulls with median:\", median_units)\n",
    "\n",
    "# Fill categorical nulls for State/Group\n",
    "for cat in ['State','state','Group','group','Category']:\n",
    "    for c in df_clean.columns:\n",
    "        if c.lower() == cat.lower():\n",
    "            df_clean[c] = df_clean[c].fillna('Unknown')\n",
    "\n",
    "# Flag missing Date rows\n",
    "df_clean['has_date'] = df_clean['Date'].notna()\n",
    "df_clean['has_time'] = df_clean['Date'].notna() | ('Time' in df_clean.columns and df_clean['Time'].notna())\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae834b",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Normalization (Min–Max)\n",
    "\n",
    "We add Min–Max normalized versions of Sales and Unit. Normalized values are useful for modeling and clustering while preserving original units for reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "norm_cols = []\n",
    "if sales_col: norm_cols.append(sales_col)\n",
    "if unit_col and unit_col not in norm_cols: norm_cols.append(unit_col)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "if norm_cols:\n",
    "    df_clean[[c+\"_norm\" for c in norm_cols]] = scaler.fit_transform(df_clean[norm_cols])\n",
    "    print(\"Added normalized columns:\", [c+\"_norm\" for c in norm_cols])\n",
    "else:\n",
    "    print(\"No columns to normalize detected.\")\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290e4b3",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Descriptive Statistics\n",
    "\n",
    "We compute mean, median, mode, standard deviation and totals for `Sales` and `Unit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats = {}\n",
    "for c in [sales_col, unit_col]:\n",
    "    if c:\n",
    "        stats[c] = {\n",
    "            'mean': df_clean[c].mean(),\n",
    "            'median': df_clean[c].median(),\n",
    "            'mode': df_clean[c].mode().iloc[0] if not df_clean[c].mode().empty else np.nan,\n",
    "            'std': df_clean[c].std(),\n",
    "            'total': df_clean[c].sum()\n",
    "        }\n",
    "import pandas as pd\n",
    "pd.DataFrame(stats).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7eb67",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Group-wise and State-wise Analysis\n",
    "\n",
    "We produce summed and averaged metrics per Group and per State to identify high-performing segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b964a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_col = None\n",
    "for candidate in ['Group','group','Category','category']:\n",
    "    for c in df_clean.columns:\n",
    "        if c.lower() == candidate.lower():\n",
    "            group_col = c\n",
    "            break\n",
    "    if group_col: break\n",
    "\n",
    "state_col = None\n",
    "for candidate in ['State','state','Region','region']:\n",
    "    for c in df_clean.columns:\n",
    "        if c.lower() == candidate.lower():\n",
    "            state_col = c\n",
    "            break\n",
    "    if state_col: break\n",
    "\n",
    "print(\"Group column:\", group_col)\n",
    "print(\"State column:\", state_col)\n",
    "\n",
    "# Aggregate by group\n",
    "if group_col and sales_col:\n",
    "    group_sales = df_clean.groupby(group_col)[sales_col].agg(['sum','mean','median','count']).sort_values('sum', ascending=False)\n",
    "    display(group_sales)\n",
    "else:\n",
    "    print(\"Group or Sales column missing; cannot aggregate by group.\")\n",
    "\n",
    "# Aggregate by state\n",
    "if state_col and sales_col:\n",
    "    state_sales = df_clean.groupby(state_col)[sales_col].agg(['sum','mean','median','count']).sort_values('sum', ascending=False)\n",
    "    display(state_sales)\n",
    "else:\n",
    "    print(\"State or Sales column missing; cannot aggregate by state.\")\n",
    "\n",
    "# Top and bottom performers\n",
    "top_group = group_sales['sum'].idxmax() if 'group_sales' in locals() and not group_sales.empty else None\n",
    "bottom_group = group_sales['sum'].idxmin() if 'group_sales' in locals() and not group_sales.empty else None\n",
    "top_state = state_sales['sum'].idxmax() if 'state_sales' in locals() and not state_sales.empty else None\n",
    "bottom_state = state_sales['sum'].idxmin() if 'state_sales' in locals() and not state_sales.empty else None\n",
    "print(\"Top group:\", top_group, \"Bottom group:\", bottom_group)\n",
    "print(\"Top state:\", top_state, \"Bottom state:\", bottom_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212c213",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Time-based Analysis\n",
    "\n",
    "We compute daily, weekly, monthly and quarterly aggregates. Rows without valid `Date` will be excluded from time-series analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31789267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if df_clean['has_date'].any():\n",
    "    df_time = df_clean[df_clean['has_date']].copy()\n",
    "    df_time.set_index('Date', inplace=True)\n",
    "    daily = df_time.resample('D')[sales_col].sum().rename('daily_sales')\n",
    "    weekly = df_time.resample('W')[sales_col].sum().rename('weekly_sales')\n",
    "    monthly = df_time.resample('M')[sales_col].sum().rename('monthly_sales')\n",
    "    quarterly = df_time.resample('Q')[sales_col].sum().rename('quarterly_sales')\n",
    "    display(daily.head(14))\n",
    "    display(weekly.head(10))\n",
    "    display(monthly.head(10))\n",
    "    display(quarterly.head(10))\n",
    "else:\n",
    "    print(\"No valid dates available for time series analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648d3f9",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Time-of-day Analysis (Hourly)\n",
    "\n",
    "We extract the hour of day from the `Date` (or `Time`) and aggregate sales to determine peak and low sales hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Derive hour of day\n",
    "if df_clean['has_date'].any():\n",
    "    df_clean['hour'] = df_clean['Date'].dt.hour\n",
    "elif 'Time' in df_clean.columns and df_clean['Time'].notna().any():\n",
    "    df_clean['Time_dt'] = pd.to_datetime(df_clean['Time'].astype(str), errors='coerce')\n",
    "    df_clean['hour'] = df_clean['Time_dt'].dt.hour\n",
    "else:\n",
    "    df_clean['hour'] = np.nan\n",
    "\n",
    "if df_clean['hour'].notna().any():\n",
    "    hourly = df_clean.groupby('hour')[sales_col].sum().sort_index()\n",
    "    display(hourly)\n",
    "else:\n",
    "    print(\"No hour information available for analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10ec89",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Visualizations\n",
    "\n",
    "Plots created in this section are saved to `/mnt/data/AAL_analysis_outputs`. They include:\n",
    "- State-wise sales bar chart (top states)  \n",
    "- Group-wise sales bar chart  \n",
    "- Daily sales line plot  \n",
    "- Hour-by-group heatmap (if possible)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "os.makedirs(r\"/mnt/data/AAL_analysis_outputs\", exist_ok=True)\n",
    "\n",
    "# State-wise sales bar chart (top 15)\n",
    "if 'state_sales' in locals() and not state_sales.empty:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(x=state_sales.index[:15], y=state_sales['sum'].values[:15])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"Total Sales\")\n",
    "    plt.title(\"State-wise Total Sales (Top states)\")\n",
    "    path_state = os.path.join(r\"/mnt/data/AAL_analysis_outputs\", \"state_sales_bar.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_state)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", path_state)\n",
    "\n",
    "# Group-wise sales bar chart\n",
    "if 'group_sales' in locals() and not group_sales.empty:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=group_sales.index, y=group_sales['sum'].values)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.ylabel(\"Total Sales\")\n",
    "    plt.title(\"Group-wise Total Sales\")\n",
    "    path_group = os.path.join(r\"/mnt/data/AAL_analysis_outputs\", \"group_sales_bar.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_group)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", path_group)\n",
    "\n",
    "# Daily sales line\n",
    "if 'daily' in locals():\n",
    "    plt.figure(figsize=(14,5))\n",
    "    daily.plot()\n",
    "    plt.ylabel(\"Daily Sales\")\n",
    "    plt.title(\"Daily Sales (Q4)\")\n",
    "    path_daily = os.path.join(r\"/mnt/data/AAL_analysis_outputs\", \"daily_sales_line.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_daily)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", path_daily)\n",
    "\n",
    "# Hour x Group heatmap\n",
    "if 'hour' in df_clean.columns and group_col and sales_col:\n",
    "    pivot = df_clean.groupby([group_col,'hour'])[sales_col].sum().unstack(fill_value=0)\n",
    "    if not pivot.empty:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.heatmap(pivot, linewidths=0.5)\n",
    "        plt.title(\"Sales by Group vs Hour of Day\")\n",
    "        path_heat = os.path.join(r\"/mnt/data/AAL_analysis_outputs\", \"group_hour_heatmap.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_heat)\n",
    "        plt.close()\n",
    "        print(\"Saved:\", path_heat)\n",
    "\n",
    "print(\"All available plots saved to:\", r\"/mnt/data/AAL_analysis_outputs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ee91e",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Recommendations & Next Steps\n",
    "\n",
    "**Immediate marketing actions**  \n",
    "- Target low-sales states (e.g., the bottom-performing state) with promotions and tailored assortments.  \n",
    "- Design targeted campaigns for low-performing groups (e.g., Seniors), including loyalty incentives and easier purchase flows.\n",
    "\n",
    "**Data/engineering**  \n",
    "- Keep both original and normalized values. Use normalized features for modeling.  \n",
    "- Implement logging to reduce 'Unknown' categorical records; capture store id, channel, and customer segment.  \n",
    "\n",
    "**Analytics**  \n",
    "- Run cohort analysis by first purchase date and retention for each Group and State.  \n",
    "- Use clustering on normalized features to identify customer segments for hyper-personalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save cleaned and normalized CSV for record-keeping\n",
    "cleaned_csv = os.path.join(r\"/mnt/data/AAL_analysis_outputs\", \"AAL_Q4_cleaned_full.csv\")\n",
    "df_clean.to_csv(cleaned_csv, index=False)\n",
    "print(\"Cleaned CSV saved to:\", cleaned_csv)\n",
    "\n",
    "# Also save normalized version (if norm cols exist)\n",
    "normalized_csv = os.path.join(r\"/mnt/data/AAL_analysis_outputs\", \"AAL_Q4_normalized_full.csv\")\n",
    "df_clean.to_csv(normalized_csv, index=False)\n",
    "print(\"Normalized CSV saved to:\", normalized_csv)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
